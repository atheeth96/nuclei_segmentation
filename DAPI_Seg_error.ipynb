{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "\n",
=======
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "import os, shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread,imsave\n",
    "from skimage import morphology\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage import img_as_uint\n",
    "\n",
    "from extractor_class.patch_extractor import PatchExtractor\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
<<<<<<< HEAD
    "from losses import SoftDiceLoss,MultiClassBCE,dice_metric\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from Models.ModelsTorch import AttnUNet,R2U_Net,R2AttU_Net,save_model,load_model\n",
=======
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from Models.ModelsTorch import AttnUNet,R2U_Net,R2AttU_Net\n",
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
    "from Generators.DatasetTorch import DataSet,ToTensor,Scale,Color,\\\n",
    "RandomHorizontalFlip,GaussianNoise,RandomCropResize,ContrasrStretching\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from preprocess import fuse_roi,generate_train_mask\n",
    "from prediction import whole_img_pred,post_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "print(tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wt=pd.read_excel('Summary_v3_15102019.xlsx',sheet_name='within_tumor')\n",
    "df_wt.columns=[x.replace(\" \",\"_\") for x in df_wt.columns]\n",
    "df_ot=pd.read_excel('Summary_v3_15102019.xlsx',sheet_name='outside_tumor')\n",
    "df_ot.columns=[x.replace(\" \",\"_\") for x in df_ot.columns]\n",
    "df_whole=pd.read_excel('Summary_v3_15102019.xlsx',sheet_name='whole_tissue')\n",
    "df_whole.columns=[x.replace(\" \",\"_\") for x in df_whole.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srikanth_data='/home/vahadaneabhi01/datalab/training-assets/R_medical/srikanth/data'\n",
    "srikanth_slides=os.listdir(srikanth_data)\n",
    "atheeth_list=[x for x in os.listdir('Dapi_patient_data/') if 'S-' in x and '.zip' not in x]\n",
    "slides_to_transfer=list(set(srikanth_slides)-(set(atheeth_list)&set(srikanth_slides)))\n",
    "# for slide in tqdm(slides_to_transfer):\n",
    "#     os.system('cp -r {} {}'.format(srikanth_data+'/'+slide,'Dapi_patient_data/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing ROIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_done=['S-190413-00208','S-190413-00163','S-190501-00409','S-190413-00113','S-190413-00054','S-190413-00238',\\\n",
    "              'S-190413-00104','S-190413-00137','S-190413-00217','S-190413-00098','S-190413-00140','S-190413-00107',\\\n",
    "              'S-190501-00428','S-190501-00422','S-190501-00425','S-190501-00425']\n",
    "\n",
    "patient_list=list(set(slides_to_transfer)-set(patients_done))#['S-190413-00146','S-190413-00110'] #'S-190413-00241','S-190413-00143',\n",
    "# for patient in patient_list:\n",
    "#     print(patient)\n",
    "#     fuse_roi(patient,'Dapi_patient_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont use S-190501-00425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# develop mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for patient in ['S-190501-00431']:\n",
    "#     generate_train_mask(patient,'Dapi_patient_data')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patient_list=[os.path.join('Dapi_patient_data',x) for x in os.listdir('Dapi_patient_data') if 'S-19' in x and x!='S-190501-00425']\n",
    "\n",
    "test_list=[os.path.join('Dapi_patient_data',x) for x in\\\n",
    "           ['S-190413-00092','S-190410-00044','S-190413-00078','S-190413-00075','S-190413-00072']]\n",
    "\n",
    "\n",
    "print(\"TEST\\n\",*test_list,sep = \"\\n\")\n",
    "train_list=[x for x in patient_list if x not in test_list]\n",
    "train_list=np.random.choice(train_list,15)\n",
    "print('\\n')\n",
    "print(\"TRAIN\\n\",*train_list,sep = \"\\n\")\n",
    "\n",
    "img_patches_dir_train='img_patches_train_15'\n",
    "nuc_pathces_dir_train='nuc_patches_train_15'\n",
    "bound_patches_dir_train='bound_patches_train_15'\n",
    "\n",
    "img_patches_dir_test='img_patches_test_5'\n",
    "nuc_pathces_dir_test='nuc_patches_test_5'\n",
    "bound_patches_dir_test='bound_patches_test_5'\n",
    "\n",
    "# train_extractor=PatchExtractor(train_list,img_patches_dir_train\\\n",
    "#                                ,nuc_pathces_dir_train,bound_patches_dir_train)\n",
    "\n",
    "# train_extractor.extract_patches()\n",
    "\n",
    "# test_extractor=PatchExtractor(test_list,img_patches_dir_test\\\n",
    "#                                ,nuc_pathces_dir_test,bound_patches_dir_test)\n",
    "\n",
    "# test_extractor.extract_patches()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  6 16:55:36 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0   285W / 300W |  15220MiB / 32480MiB |     62%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First wave\n",
    "\n",
    "Train List\n",
    "\n",
    "S-190413-00092<br>\n",
    "S-190410-00044<br>\n",
    "S-190413-00078<br>\n",
    "S-190413-00075\n",
    "\n",
    "\n",
    "Test List\n",
    "S-190413-00081<br>\n",
    "S-190413-00072\n",
    "\n",
    "# Second wave\n",
    "\n",
    "TEST\n",
    "\n",
    "S-190413-00092<br>\n",
    "S-190410-00044<br>\n",
    "S-190413-00078<br>\n",
    "S-190413-00075<br>\n",
    "S-190413-00072\n",
    "\n",
    "\n",
    "TRAIN\n",
    "\n",
    "S-190501-00431<br>\n",
    "S-190413-00110<br>\n",
    "S-190413-00140<br>\n",
    "S-190413-00095<br>\n",
    "S-190413-00134<br>\n",
    "S-190413-00081<br>\n",
    "S-190413-00054<br>\n",
    "S-190413-00208<br>\n",
    "S-190413-00054<br>\n",
    "S-190413-00208<br>\n",
    "S-190413-00146<br>\n",
    "S-190413-00104<br>\n",
    "S-190413-00208<br>\n",
    "S-190413-00208<br>\n",
    "S-190413-00098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and push to Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train=8\n",
    "batch_size_test=4\n",
    "augmentation_transform=[GaussianNoise(),RandomCropResize(),ContrasrStretching()]\n",
    "add_augmentaion=False\n",
    "if add_augmentaion: \n",
    "    transform=torchvision.transforms.Compose([Scale(input_image=True),\\\n",
    "                                                  torchvision.transforms.RandomApply(augmentation_transform, p=0.35)\\\n",
    "                                                  ,ToTensor()])\n",
    "else:\n",
    "    transform=torchvision.transforms.Compose([Scale(scale_type='maximum'),ToTensor()])    \n",
    "test_transform=torchvision.transforms.Compose([Scale(scale_type='maximum'),ToTensor()])\n",
    "\n",
    "\n",
    "train_dataset=DataSet('img_patches_train_15','nuc_patches_train_15','bound_patches_train_15',transform=transform)\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size_train,num_workers=0,shuffle=True)\n",
    "print(train_dataset.__len__(),\" Train samples\")\n",
    "\n",
    "test_dataset=DataSet('img_patches_test_5','nuc_patches_test_5','bound_patches_test_5',transform=test_transform)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size_test,num_workers=0,shuffle=False)\n",
    "print(test_dataset.__len__(),\" Test samples\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loader(loader,index):\n",
    "    for i,sample in enumerate(loader):\n",
    "        #print(sample['image'].shape)\n",
    "        if i==1:\n",
    "            image=(sample['image'][index]).numpy()\n",
    "            \n",
    "            \n",
    "            #image=np.zeros(image_i.shape,dtype=np.uint8)\n",
    "            #image[np.where(image_i!=0)]=255\n",
    "\n",
    "            mask=(sample['nuclei_mask'][index]).numpy()\n",
    "            \n",
    "        \n",
    "            boundary=(sample['bound_mask'][index]).numpy()\n",
    "            output=sample['nuclei_mask']\n",
    "            output=torch.cat((output,sample['nuclei_mask']),dim=1)\n",
    "           \n",
    "    \n",
    "            image=np.squeeze(image.transpose(1,2,0),axis=2)\n",
    "            print(image.shape,np.amax(image))\n",
    "            mask=np.squeeze(mask.transpose(1,2,0),axis=2) \n",
    "            \n",
    "            boundary=np.squeeze(boundary.transpose(1,2,0),axis=2)\n",
    "            #print(image)\n",
    "            fig=plt.figure()\n",
    "            plt.imshow(image*255)\n",
    "            fig2=plt.figure()\n",
    "            plt.imshow(mask)\n",
    "            fig3=plt.figure()\n",
    "            plt.imshow(boundary)\n",
    "            break\n",
    "visualize_loader(test_loader,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zipFolder(folder_path,zipFile_name):\n",
    "    file_paths=[os.path.join(folder_path,x) for x in os.listdir(folder_path)]\n",
    "    with ZipFile('{}.zip'.format(zipFile_name),'w') as zip: \n",
    "            # writing each file one by one \n",
    "            for file in file_paths: \n",
    "                zip.write(file) \n",
    "    print(\"DONE ZIPPING\")\n",
    "            \n",
    "def unzipFile(zipFile_path,dest_folder=os.getcwd()):\n",
    "    with ZipFile(zipFile_path, 'r') as zipObj:\n",
    "        zipObj.extractall()\n",
    "        print(\"DONE UNZIPPING\")\n",
    "        \n",
    "def make_archive(source, destination):\n",
    "    base = os.path.basename(destination)\n",
    "    name = base.split('.')[0]\n",
    "    format = base.split('.')[1]\n",
    "    archive_from = os.path.dirname(source)\n",
    "    archive_to = os.path.basename(source.strip(os.sep))\n",
    "    print(source, destination, archive_from, archive_to)\n",
    "    shutil.make_archive(name, format, archive_from, archive_to)\n",
    "    shutil.move('%s.%s'%(name,format), destination)\n",
    "        \n",
    "if not os.path.exists('NucSeg'):        \n",
    "    unzipFile('NucSeg.zip')\n",
    "else:\n",
    "    print(\"NucSeg has already been unzipped\")\n",
    "\n",
    "if not os.path.exists('DAPI'):        \n",
    "    unzipFile('DAPI.zip')\n",
    "else:\n",
    "    print(\"DAPI has already been unzipped\")\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip entire dir and sub-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.make_archive('digest', 'zip', '/datalab/digest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        #probs = F.sigmoid(logits)\n",
    "        m1 = logits.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score\n",
    "    \n",
    "class MultiClassBCE(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        cat_class=targets.size(1)\n",
    "        m1 = logits.view(num,cat_class, -1)\n",
    "        m2 = targets.view(num,cat_class, -1)\n",
    "        final_loss=0\n",
    "        loss_list=[]\n",
    "        weights=[0.6,0.4]\n",
    "        for cat in range(cat_class):\n",
    "            if cat==1:\n",
    "                loss=nn.BCELoss()(m1[:,cat,:],m2[:,cat,:])\n",
    "            else:\n",
    "                loss=nn.BCELoss()(m1[:,cat,:],m2[:,cat,:])\n",
    "            final_loss+=weights[cat]*loss\n",
    "            \n",
    "\n",
    "        return final_loss\n",
    "    \n",
    "def dice_metric(y_pred,y_true):\n",
    "    smooth = 1\n",
    "    num = y_true.size(0)\n",
    "    categories=y_true.size(1)\n",
    "    m1 = y_pred.view(num,categories, -1)\n",
    "    m2 = y_true.view(num,categories, -1)\n",
    "    weights=[0.5,0.5]\n",
    "    final_score=0\n",
    "    score_list=[]\n",
    "    for cat in range(categories):\n",
    "        \n",
    "        \n",
    "        intersection = (m1[:,cat,:] * m2[:,cat,:])\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1[:,cat,:].sum(1) + m2[:,cat,:].sum(1) + smooth)\n",
    "        score = score.sum() / num\n",
    "        score=score.detach().item()\n",
    "        final_score+=score*weights[cat]\n",
    "        score_list.append(score)\n",
    "    return final_score,score_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AttnUNet(img_ch=1,output_ch=2)\n",
    "# model=R2U_Net(img_ch=1,output_ch=2,t=2)\n",
    "model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "BEST_MODEL_PATH=os.path.join(os.getcwd(),'model_{}'.format(model_start_date))\n",
    "if not os.path.exists(BEST_MODEL_PATH):\n",
    "    os.mkdir(BEST_MODEL_PATH)\n",
    "    print('model_{} dir has been made'.format(model_start_date))\n",
    "print(\"Model's state_dict:\")\n",
    "writer = SummaryWriter('model_{}/dapi_seg_experiment_{}'.format(model_start_date,1))\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "images= next(iter(train_loader))\n",
    "model = model.to(device)\n",
    "writer.add_graph(model,images['image'].to(device, dtype = torch.float))\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "optimizer_selected='adam'\n",
    "batchsize=4\n",
    "no_steps=train_dataset.__len__()//batchsize\n",
    "restart_epochs=8\n",
    "num_epochs=10\n",
    "\n",
    "\n",
    "#criterion = SoftDiceLoss()#\n",
    "criterion=MultiClassBCE()\n",
    "\n",
    "history={'train_loss':[],'test_loss':[],'train_dice':[],'test_dice':[]}\n",
    "if optimizer_selected=='adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=10e-03, betas=(0.9, 0.98))#,weight_decay=0.02)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=10e-03, momentum=0.8,nesterov=True)\n",
    "\n",
    "scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, restart_epochs*no_steps,\\\n",
    "                                                     eta_min=10e-012, last_epoch=-1)\n",
    "\n",
    "\n",
    "best_val=0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(\"Learning Rate : {}\".format(optimizer.state_dict()['param_groups'][-1]['lr']))\n",
    "    # loop over the dataset multiple times\n",
    "    \n",
    "    run_avg_train_loss=0\n",
    "    run_avg_train_dice=0\n",
    "    \n",
    "    run_avg_train_dice_nuclei=0\n",
    "    run_avg_train_dice_bound=0\n",
    "    run_avg_test_dice_nuclei=0\n",
    "    run_avg_test_dice_bound=0\n",
    "    \n",
    "    run_avg_test_loss=0\n",
    "    run_avg_test_dice=0\n",
    "    \n",
    "    for mode in ['train','eval']:\n",
    "     \n",
    "        if mode == 'train':\n",
    "            \n",
    "            model.train()\n",
    "            loop=tqdm(train_loader)\n",
    "            \n",
    "            for i, sample_batched in (enumerate(loop)):\n",
    "                loop.set_description('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "                \n",
    "                #Clear Gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # get the inputs; data is a list of [dapi, nuclei, boundary]\n",
    "                images_batch, nuc_mask_batch,bound_mask_batch = sample_batched['image'],\\\n",
    "                sample_batched['nuclei_mask'],sample_batched['bound_mask']\n",
    "                nuclei_mask_batch=torch.cat((nuc_mask_batch,bound_mask_batch),dim=1)\n",
    "             \n",
    "                images_batch, nuclei_mask_batch = images_batch.to(device, dtype = torch.float)\\\n",
    "                ,nuclei_mask_batch.to(device, dtype = torch.float)#,bound_mask_batch.to(device,dtype=torch.float)\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = torch.sigmoid(model(images_batch))\n",
    "                \n",
    "                loss = criterion(outputs, nuclei_mask_batch)\n",
    "                dice_score,dice_list=dice_metric(outputs,nuclei_mask_batch)\n",
    "                run_avg_train_loss=(run_avg_train_loss*(0.9))+loss.detach().item()*0.1\n",
    "                run_avg_train_dice=(run_avg_train_dice*(0.9))+dice_score*0.1\n",
    "                run_avg_train_dice_nuclei=(run_avg_train_dice_nuclei*(0.9))+dice_list[0]*0.1\n",
    "                run_avg_train_dice_bound=(run_avg_train_dice_bound*(0.9))+dice_list[1]*0.1\n",
    "                if i%100==99:\n",
    "                    predicted_nuclei_train,predicted_boundary_train=torch.chunk(outputs,2,dim=1)\n",
    "                    gt_nuclei,gt_boundary=torch.chunk(nuclei_mask_batch.detach().cpu(),2,dim=1)\n",
    "                    \n",
    "                    img_tensor=torch.cat((predicted_nuclei_train.detach().cpu()\\\n",
    "                                         ,gt_nuclei\\\n",
    "                                          ,predicted_boundary_train.detach().cpu(),gt_boundary),axis=0)\n",
    "                    \n",
    "                    \n",
    "                    #print(img_tensor.shape)\n",
    "                    img_grid2 = torchvision.utils.make_grid(img_tensor,nrow=batch_size_train,padding=100)\n",
<<<<<<< HEAD
    "                    torchvision.utils.save_image\\\n",
    "                    (img_grid2,os.path.join(BEST_MODEL_PATH,\\\n",
    "                                            'train_iter_{}.png'.format(epoch*len(train_loader)+i+1)))\n",
=======
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
    "                    writer.add_image('TRAIN_EPOCH_{}_ITER_{}'.format(epoch,i), img_grid2)\n",
    "                    writer.add_histogram('First conv weights',model.Conv1.conv[0].weight,i)\n",
    "                    \n",
    "                    writer.add_scalar('Training dice score nuclei',\n",
    "                            run_avg_train_dice_nuclei,\n",
    "                            epoch * len(train_loader) + i)\n",
    "                    writer.add_scalar('Training dice score boundary',\n",
    "                            run_avg_train_dice_bound,\n",
    "                            epoch * len(train_loader) + i)\n",
    "                    writer.add_scalar('Training Loss',\n",
    "                            run_avg_train_loss,\n",
    "                            epoch * len(train_loader) + i)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                \n",
    "                loop.set_postfix(loss=run_avg_train_loss,dice_score=run_avg_train_dice,\\\n",
    "                                 nuclei_dice=run_avg_train_dice_nuclei,\\\n",
    "                                boundary_dice=run_avg_train_dice_bound)\n",
    "               \n",
    "            history['train_loss'].append(run_avg_train_loss)\n",
    "            history['train_dice'].append(run_avg_train_dice)\n",
    "                \n",
    "                 \n",
    "                    \n",
    "        elif mode =='eval':\n",
    "            #Clear Gradients\n",
    "            optimizer.zero_grad()\n",
    "            samples_test=len(test_loader)\n",
    "            model.eval()\n",
    "            val_loss=0\n",
    "            test_agg=0\n",
    "            for j, test_sample in enumerate(test_loader):\n",
    "\n",
    "                test_images_batch, test_nuclei_mask_batch,test_bound_mask_batch = test_sample['image'], test_sample['nuclei_mask'],\\\n",
    "                test_sample['bound_mask']\n",
    "                test_mask_batch=torch.cat((test_nuclei_mask_batch,test_bound_mask_batch),dim=1)\n",
    "                test_images_batch, test_mask_batch = test_images_batch.to(device, dtype = torch.float),\\\n",
    "                test_mask_batch.to(device, dtype = torch.float)\n",
    "                test_outputs = torch.sigmoid(model(test_images_batch))\n",
    "                \n",
    "                test_loss = criterion(test_outputs, test_mask_batch)\n",
    "                #final_test_loss+=test_loss.detach().item()\n",
    "                test_dice,test_dice_list=dice_metric(test_outputs,test_mask_batch)\n",
    "                #final_test_dice+=test_dice\n",
    "                run_avg_test_loss=(run_avg_test_loss*(0.9))+test_loss.detach().item()*0.1\n",
    "                run_avg_test_dice=(run_avg_test_dice*(0.9))+test_dice*0.1\n",
    "                run_avg_test_dice_nuclei=run_avg_test_dice_nuclei*0.9+test_dice_list[0]*0.1\n",
    "                run_avg_test_dice_bound=run_avg_test_dice_bound*0.9+test_dice_list[1]*0.1\n",
    "                if j%100==99:\n",
    "                    predicted_nuclei_test,predicted_boundary_test=torch.chunk(test_outputs.detach(),2,dim=1)\n",
    "                    gt_nuclei_test,gt_boundary_test=torch.chunk(test_mask_batch.detach().cpu(),2,dim=1)\n",
    "                    \n",
    "                    img_tensor_test=torch.cat((predicted_nuclei_test.detach().cpu()\\\n",
    "                                              ,gt_nuclei_test\\\n",
    "                                               ,predicted_boundary_test.detach().cpu(),gt_boundary_test),axis=0)\n",
<<<<<<< HEAD
    "                \n",
    "                    \n",
    "                    img_grid = torchvision.utils.make_grid(img_tensor_test,nrow=batch_size_test,padding=10)\n",
    "                    torchvision.utils.save_image\\\n",
    "                    (img_grid,os.path.join(BEST_MODEL_PATH,\\\n",
    "                                            'test_iter_{}.png'.format(epoch*len(train_loader)+j+1)))\n",
=======
    "                    \n",
    "                    \n",
    "                    img_grid = torchvision.utils.make_grid(img_tensor_test,nrow=batch_size_test,padding=10)\n",
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
    "                    writer.add_image('TEST_EPOCH_{}_ITER_{}'.format(epoch,j), img_grid)\n",
    "                    writer.add_scalar('Testing dice score nuclei',\\\n",
    "                                      run_avg_test_dice_nuclei,epoch * len(test_loader) + j)\n",
    "                    writer.add_scalar('Testing dice score boundary',\\\n",
    "                                      run_avg_test_dice_bound,epoch * len(test_loader) + j)\n",
    "                    writer.add_scalar('Testing Loss',\\\n",
    "                                      run_avg_test_loss,epoch * len(test_loader) + j)\n",
    "                    writer.flush()\n",
    "                \n",
    "            print(\"test_loss: {}\\ntest_dice :{}, list [nuclei,boundary] :{}\"\\\n",
    "                  .format(run_avg_test_loss,run_avg_test_dice,[run_avg_test_dice_nuclei,run_avg_test_dice_bound]))\n",
    "            history['test_loss'].append(run_avg_test_loss)\n",
    "            history['test_dice'].append(run_avg_test_dice)\n",
    "            if run_avg_test_dice>best_val:\n",
    "                best_val=run_avg_test_dice\n",
<<<<<<< HEAD
    "                save_model(model,optimizer,BEST_MODEL_PATH+'/model_optim.pth',scheduler=scheduler)\n",
    "            \n",
    "                print(\"saved model with test dice score: {}\".format(best_val))\n",
    "\n",
    "    \n",
    "save_model(model,optimizer,BEST_MODEL_PATH+'/model_final.pth',scheduler=scheduler)\n",
=======
    "                torch.save(model.state_dict(), BEST_MODEL_PATH+'/model_optim.pth')\n",
    "                print(\"saved model with test dice score: {}\".format(best_val))\n",
    "\n",
    "        \n",
    "#             print(\"val_loss {}\".format(val_loss/samples_test))\n",
    "torch.save(model.state_dict(), BEST_MODEL_PATH+'/model_final.pth')\n",
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
    "        \n",
    "        \n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
=======
>>>>>>> d647ffaee8a2ff8a96d5b0660ee8054652d245e2
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and stitch back whole image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def whole_dice_metric(y_pred,y_true):\n",
    "#     smooth = 1\n",
    "#     num = y_true.shape[0]\n",
    "#     categories=y_true.shape[1]\n",
    "#     m1 =np.reshape(y_pred,(num,categories, -1))\n",
    "#     m2 = np.reshape(y_true,(num,categories, -1))\n",
    "#     weights=[0.5,0.5]\n",
    "#     final_score=0\n",
    "#     score_list=[]\n",
    "#     for cat in range(categories):\n",
    "        \n",
    "        \n",
    "#         intersection = (m1[:,cat,:] * m2[:,cat,:])\n",
    "\n",
    "#         score = 2. * (np.sum(intersection,axis=1) + smooth) / (np.sum(m1[:,cat,:],axis=1) +\\\n",
    "#                                                                (np.sum(m2[:,cat,:],axis=1) + smooth))\n",
    "        \n",
    "#         score = np.sum(score) / num\n",
    "        \n",
    "#         final_score+=score*weights[cat]\n",
    "#         score_list.append(score)\n",
    "#     return final_score,score_list\n",
    "\n",
    "# def whole_img_pred(IMAGE_PATH,img_list,pred_dir_name,model):\n",
    "    \n",
    "    \n",
    "#     pred_dir=os.path.join(os.getcwd(),pred_dir_name)\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model=model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     if not os.path.exists(pred_dir):\n",
    "#         os.mkdir(pred_dir)\n",
    "#         print(\"Made {} directory\".format(pred_dir.split('/')[-1]))\n",
    "#     else:\n",
    "#         print(\"{} directory already exists in {}\".format(pred_dir.split('/')[-1],'/'.join(pred_dir.split('/')[:-1])))\n",
    "        \n",
    "#     step=512\n",
    "#     avg_dice=0\n",
    "#     loop=tqdm(img_list)\n",
    "#     for count,img_name in enumerate(loop):\n",
    "        \n",
    "        \n",
    "        \n",
    "#         input_image=imread(os.path.join(IMAGE_PATH+'/ROI',img_name))\n",
    "#         boundary_mask=imread(os.path.join(IMAGE_PATH+'/bound_mask',img_name))\n",
    "#         nuclei_mask=imread(os.path.join(IMAGE_PATH+'/nuc_mask',img_name))\n",
    "        \n",
    "#         y_true=np.stack((nuclei_mask,boundary_mask),axis=0)\n",
    "#         y_true=np.expand_dims(y_true,axis=0)\n",
    "    \n",
    "#         r,c=input_image.shape#4663,3881\n",
    "\n",
    "#         new_r_count=(math.ceil((r-512)/512)+1)#5\n",
    "#         new_c_count=(math.ceil((c-512)/512)+1)#5\n",
    "\n",
    "\n",
    "#         pad_r1=((new_r_count-1)*512-r+512)//2 #200\n",
    "#         pad_r2=((new_r_count-1)*512-r+512)-pad_r1 #200\n",
    "#         pad_c1=((new_c_count-1)*512-c+512)//2 #0\n",
    "#         pad_c2=((new_c_count-1)*512-c+512)-pad_c1#0\n",
    "\n",
    "#         image_padded=np.pad(input_image, [(pad_r1,pad_r2),(pad_c1,pad_c2)], 'constant', constant_values=0)/np.amax(input_image)\n",
    "\n",
    "\n",
    "#         window_shape=(512,512)\n",
    "\n",
    "#         img_patches=skimage.util.view_as_windows(image_padded, window_shape, step=step)\n",
    "#         img_patches=img_patches.reshape((-1,512,512))\n",
    "#         img_patches=img_patches.transpose((0,2,1))\n",
    "#         img_patches=np.expand_dims(img_patches,axis=1)\n",
    "        \n",
    "    \n",
    "#         bound_temp=[]\n",
    "#         nuclei_temp=[]\n",
    "        \n",
    "#         for i in range(new_r_count):\n",
    "            \n",
    "#             temp_img_patches=torch.from_numpy(img_patches[i*new_r_count:(i+1)*new_r_count]).type(torch.FloatTensor).to(device)\n",
    "#             pred=torch.sigmoid(model(temp_img_patches))\n",
    "#             del temp_img_patches\n",
    "            \n",
    "#             nuclei,bound=torch.chunk(pred,2,dim=1)\n",
    "#             del pred\n",
    "#             nuclei,bound=nuclei.detach().cpu().numpy(),bound.detach().cpu().numpy()\n",
    "            \n",
    "#             nuclei=np.squeeze(nuclei,axis=1).transpose((0,2,1))\n",
    "#             nuclei=np.concatenate(nuclei,axis=1)\n",
    "#             nuclei_temp.append(nuclei)\n",
    "            \n",
    "#             bound=np.squeeze(bound,axis=1).transpose((0,2,1))\n",
    "#             bound=np.concatenate(bound,axis=1)\n",
    "#             bound_temp.append(bound)\n",
    "            \n",
    "#         nuclei_temp=np.array(nuclei_temp)\n",
    "#         nuclei_temp=np.concatenate(nuclei_temp,axis=0)\n",
    "        \n",
    "#         bound_temp=np.array(bound_temp)\n",
    "#         bound_temp=np.concatenate(bound_temp,axis=0)\n",
    "        \n",
    "#         nuclei_temp=nuclei_temp[pad_r1:nuclei_temp.shape[0]-pad_r2,pad_c1:nuclei_temp.shape[1]-pad_c2]*255\n",
    "#         bound_temp=bound_temp[pad_r1:bound_temp.shape[0]-pad_r2,pad_c1:bound_temp.shape[1]-pad_c2]*255\n",
    "        \n",
    "#         nuclei_temp=nuclei_temp.astype(np.uint8)\n",
    "#         bound_temp=bound_temp.astype(np.uint8)\n",
    "        \n",
    "#         y_pred=np.stack((nuclei_temp,bound_temp),axis=0)\n",
    "#         y_pred=np.expand_dims(y_pred,axis=0)\n",
    "        \n",
    "#         dice_score,dice_list=whole_dice_metric(y_pred,y_true)\n",
    "        \n",
    "#         loop.set_postfix(Dice_score =dice_score, nuclei_dice=dice_list[0], boundary_dice=dice_list[1])\n",
    "#         avg_dice+=dice_score\n",
    "    \n",
    "        \n",
    "        \n",
    "#         imsave(pred_dir_name+'/bound_'+img_name.split('.')[0]+'.png',bound_temp)\n",
    "        \n",
    "#         imsave(pred_dir_name+'/nuclei_'+img_name.split('.')[0]+'.png',nuclei_temp)\n",
    "\n",
    "        \n",
    "    \n",
    "#     print(\"DONE\\nAverage Dice Score = {}\".format(avg_dice/(count+1)))\n",
    "    \n",
    "    \n",
    "#model=R2U_Net(img_ch=1,output_ch=2,t=2)\n",
    "model=AttnUNet(img_ch=1,output_ch=2)\n",
    "#print(model)\n",
    "patient_name='S-190413-00241'\n",
    "model.load_state_dict(torch.load('model_2019_11_13/model_optim.pth'))\n",
    "IMAGE_PATH='/datalab/training-assets/R_medical/DAPI/Dapi_patient_data/{}'.format(patient_name)\n",
    "pred_dir_name=os.path.join(IMAGE_PATH,'predictions')\n",
    "#whole_img_pred(IMAGE_PATH,os.listdir(os.path.join(IMAGE_PATH,'ROI')),pred_dir_name,model)\n",
    "#zipFolder('Dapi_patient_data/{}/predictions/'.format(patient_name),'{}_predictions'.format(patient_name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def multiple_erosion(img,iter=5):\n",
    "#     for j in (range(5)):\n",
    "#         img=morphology.binary_erosion(img, selem=morphology.selem.disk(1))\n",
    "#     return img\n",
    "\n",
    "# def multiple_dialte(img,iter=5):\n",
    "#     for j in (range(5)):\n",
    "#         img=morphology.binary_dilation(img, selem=morphology.selem.disk(1))\n",
    "#     return img\n",
    "\n",
    "# def post_process(PRED_PATH,img_list,processed_dir):\n",
    "#     if not os.path.exists(processed_dir):\n",
    "#         os.mkdir(processed_dir)\n",
    "#     avg_error=0\n",
    "#     for i,img_name in enumerate(img_list):\n",
    "#         bound_img_path=os.path.join(PRED_PATH,'bound_'+img_name.split('.')[0]+'.png')\n",
    "#         bound_img=imread(bound_img_path)\n",
    "    \n",
    "#         thresh_bound=threshold_otsu(bound_img)\n",
    "#         bound_img=bound_img>thresh_bound\n",
    "        \n",
    "#         nuc_img_path=os.path.join(PRED_PATH,'nuclei_'+img_name.split('.')[0]+'.png')\n",
    "#         nuc_img=imread(nuc_img_path)\n",
    "        \n",
    "#         thresh_nuclei=threshold_otsu(nuc_img)\n",
    "#         nuc_img=nuc_img>thresh_nuclei\n",
    "        \n",
    "    \n",
    "#         bound_img=multiple_dialte(bound_img)\n",
    "#         bound_img=multiple_erosion(bound_img)\n",
    "        \n",
    "#         nuc_img=multiple_erosion(nuc_img)\n",
    "#         nuc_img=multiple_dialte(nuc_img)\n",
    "        \n",
    "#         comb_img=nuc_img^bound_img\n",
    "#         bound_coor=np.where(bound_img==1)\n",
    "#         comb_img[bound_coor]=0\n",
    "#         comb_img=multiple_erosion(comb_img,3)\n",
    "#         comb_img=multiple_dialte(comb_img,3)\n",
    "        \n",
    "#         gt=int(df_whole[(df_whole['ROI']==img_name.split('.')[0]) & (df_whole['Slide']==patient_name)][\"ALLCELLS\"])\n",
    "        \n",
    "#         imsave(processed_dir+'/'+img_name,img_as_uint(comb_img))\n",
    "#         labels=skimage.measure.label(comb_img)\n",
    "#         print(\"{} {} ,GT {},\\n\".format(img_name,np.max(labels),gt)\\\n",
    "#               +Color.RED+\"Error rate {}\".format((gt-np.max(labels))/gt)+Color.END)\n",
    "#         avg_error+=(gt-np.max(labels))/gt\n",
    "#     print(\"avg error : \",avg_error/(i+1))\n",
    "        \n",
    "\n",
    "img_list=os.listdir(IMAGE_PATH+'/ROI')\n",
    "PRED_PATH=os.path.join(IMAGE_PATH,'predictions')\n",
    "processed_dir=os.path.join(IMAGE_PATH,'processed')\n",
    "post_process(PRED_PATH,img_list,processed_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH='Dapi_patient_data/'\n",
    "patient_list=[x for x in os.listdir(BASE_PATH) if 'S-' in x and '.zip' not in x]\n",
    "\n",
    "def tabulate_anomalies(BASE_PATH,patient_list)\n",
    "    patient_list.sort()\n",
    "    print(patient_list)\n",
    "    count=0\n",
    "    df_new=pd.DataFrame(columns=['Slide','ROI','Nuclei_count_in_whole_tissue'\\\n",
    "                     ,'Nuclei_count_in_within_tumor','Nuclei_count_in_outside_tissue',\\\n",
    "                    'Nuclei_count_in_images',])\n",
    "    for patient in tqdm(patient_list):\n",
    "    roi_list=os.listdir(os.path.join(BASE_PATH,patient+'/ROI'))\n",
    "    roi_list.sort()\n",
    "    ground_truth_list=os.listdir(os.path.join(BASE_PATH,patient+'/NucSeg'))\n",
    "        for roi in roi_list:\n",
    "            ground_truth_path=os.path.join(os.path.join(BASE_PATH,patient+'/NucSeg'),\\\n",
    "                                           [x for x in ground_truth_list if roi.split('.')[0] in x][-1])\n",
    "        #         print(os.path.join(BASE_PATH,patient+'/NucSeg'),\\\n",
    "        #                                        [x for x in ground_truth_list if roi.split('.')[0] in x][-1],j)\n",
    "            ground_truth=imread(ground_truth_path)\n",
    "            all_cells_gt=np.amax(ground_truth)\n",
    "            roi_name=roi.split('.')[0]\n",
    "            #print(roi_name,patient)\n",
    "            all_cells_wt=int(df_wt[(df_wt['ROI']==roi_name) & (df_wt['Slide']==patient)][\"ALLCELLS\"])\n",
    "            all_cells_ot=int(df_ot[(df_ot['ROI']==roi_name) & (df_ot['Slide']==patient)][\"ALLCELLS\"])\n",
    "            all_cells_whole=int(df_whole[(df_whole['ROI']==roi_name) & (df_whole['Slide']==patient)][\"ALLCELLS\"])\n",
    "\n",
    "\n",
    "            df_new.loc[count]=[patient,roi_name,all_cells_whole,all_cells_wt,all_cells_ot,all_cells_gt]\n",
    "            count+=1\n",
    "    df_new.to_csv('anomalies.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
